{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "irzex3XrDwiA",
        "outputId": "ecad0aea-02db-4942-dd02-65a3a5107b7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2023 NVIDIA Corporation\n",
            "Built on Tue_Aug_15_22:02:13_PDT_2023\n",
            "Cuda compilation tools, release 12.2, V12.2.140\n",
            "Build cuda_12.2.r12.2/compiler.33191640_0\n"
          ]
        }
      ],
      "source": [
        "!nvcc --version"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile example.cu\n",
        "#include <stdio.h>\n",
        "\n",
        "__global__ void helloFromGPU() {\n",
        "    printf(\"Hello World from GPU!\\n\");\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    helloFromGPU<<<1, 10>>>();\n",
        "    cudaDeviceSynchronize();\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XZ5XYRRCFTwE",
        "outputId": "1f1135a4-81ae-4767-9182-317b50ad008a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing example.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -o example example.cu\n",
        "!./example"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b-C2qwutFa_j",
        "outputId": "13e02654-00b1-4a2a-dc47-eeca326dd547"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello World from GPU!\n",
            "Hello World from GPU!\n",
            "Hello World from GPU!\n",
            "Hello World from GPU!\n",
            "Hello World from GPU!\n",
            "Hello World from GPU!\n",
            "Hello World from GPU!\n",
            "Hello World from GPU!\n",
            "Hello World from GPU!\n",
            "Hello World from GPU!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile Memory_Allocation.cu\n",
        "#include \"cuda_runtime.h\"\n",
        "#include \"device_launch_parameters.h\"\n",
        "#include <stdio.h>\n",
        "\n",
        "void checkDeviceMemory(void)\n",
        "{\n",
        "  size_t free, total;\n",
        "  cudaMemGetInfo(&free, &total);\n",
        "  printf(\"Device memory (free/total) = %lld/%lld bytes\\n\", free, total);\n",
        "}\n",
        "\n",
        "int main() {\n",
        "  int* dDataPtr;\n",
        "  cudaError_t errorCode;\n",
        "\n",
        "  checkDeviceMemory();\n",
        "  errorCode = cudaMalloc(&dDataPtr, sizeof(int) * 1024 * 1024); // 4MB 할당\n",
        "  printf(\"cudaMalloc - %s\\n\", cudaGetErrorName(errorCode));\n",
        "  checkDeviceMemory();\n",
        "\n",
        "  errorCode = cudaMemset(dDataPtr, 0, sizeof(int) * 1024 * 1024); // 모두 0으로 초기화\n",
        "  printf(\"cudaMemset - %s\\n\", cudaGetErrorName(errorCode));\n",
        "\n",
        "  errorCode = cudaFree(dDataPtr); // 할당된 메모리 공간 해제\n",
        "  printf(\"cudaFree - %s\\n\", cudaGetErrorName(errorCode));\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p0ApKgGO12dM",
        "outputId": "a3ecbfd8-62b3-4e89-967d-f825d07f6412"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing Memory_Allocation.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -o Memory_Allocation Memory_Allocation.cu\n",
        "!./Memory_Allocation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jgqCOT1L_GfL",
        "outputId": "737d435a-414e-4a85-fa64-f9d336fcc9ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[01m\u001b[KMemory_Allocation.cu:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kvoid checkDeviceMemory()\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[KMemory_Allocation.cu:9:8:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kformat ‘\u001b[01m\u001b[K%lld\u001b[m\u001b[K’ expects argument of type ‘\u001b[01m\u001b[Klong long int\u001b[m\u001b[K’, but argument 2 has type ‘\u001b[01m\u001b[Ksize_t\u001b[m\u001b[K’ {aka ‘\u001b[01m\u001b[Klong unsigned int\u001b[m\u001b[K’} [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wformat=\u0007-Wformat=\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "    9 |   print\u001b[01;35m\u001b[Kf(\"Device memory (free/total) = %lld/%lld bytes\\\u001b[m\u001b[Kn\"\u001b[32m\u001b[K, fr\u001b[m\u001b[Kee, total);\n",
            "      |        \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K  \u001b[32m\u001b[K~~~~\u001b[m\u001b[K\n",
            "      |                                                          \u001b[32m\u001b[K|\u001b[m\u001b[K\n",
            "      |                                                          \u001b[32m\u001b[Ksize_t {aka long unsigned int}\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[KMemory_Allocation.cu:9:8:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kformat ‘\u001b[01m\u001b[K%lld\u001b[m\u001b[K’ expects argument of type ‘\u001b[01m\u001b[Klong long int\u001b[m\u001b[K’, but argument 3 has type ‘\u001b[01m\u001b[Ksize_t\u001b[m\u001b[K’ {aka ‘\u001b[01m\u001b[Klong unsigned int\u001b[m\u001b[K’} [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wformat=\u0007-Wformat=\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "    9 |   print\u001b[01;35m\u001b[Kf(\"Device memory (free/total) = %lld/%lld bytes\\\u001b[m\u001b[Kn\", free\u001b[32m\u001b[K, tot\u001b[m\u001b[Kal);\n",
            "      |        \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K        \u001b[32m\u001b[K~~~~~\u001b[m\u001b[K\n",
            "      |                                                                \u001b[32m\u001b[K|\u001b[m\u001b[K\n",
            "      |                                                                \u001b[32m\u001b[Ksize_t {aka long unsigned int}\u001b[m\u001b[K\n",
            "Device memory (free/total) = 15727656960/15835660288 bytes\n",
            "cudaMalloc - cudaSuccess\n",
            "Device memory (free/total) = 15723462656/15835660288 bytes\n",
            "cudaMemset - cudaSuccess\n",
            "cudaFree - cudaSuccess\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile Memcpy.cu\n",
        "#include \"cuda_runtime.h\"\n",
        "#include \"device_launch_parameters.h\"\n",
        "#include <stdio.h>\n",
        "\n",
        "__global__ void printData(int* _dDataPtr) {\n",
        "  printf(\"%d\", _dDataPtr[threadIdx.x]);\n",
        "} // printData 커널(스레드별 자신이 담당하는 원소의 번호가 threadIdx.x가 됨)\n",
        "\n",
        "\n",
        "__global__ void setData(int* _dDataPtr) {\n",
        "  _dDataPtr[threadIdx.x] = 2;\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int data[10] = {0};\n",
        "    for (int i = 0; i < 10; i++) data[i] = 1;\n",
        "\n",
        "    int* dDataPtr;\n",
        "    cudaMalloc(&dDataPtr, sizeof(int) * 10); // 40byte\n",
        "    cudaMemset(dDataPtr, 0, sizeof(int) * 10); // 0으로 초기화\n",
        "\n",
        "    printf(\"Data in device: \");\n",
        "    printData <<<1, 10>>> (dDataPtr);\n",
        "\n",
        "    cudaMemcpy(dDataPtr, data, sizeof(int) * 10, cudaMemcpyHostToDevice);\n",
        "    // data(host)로부터 dDataPtr(device)로 40byte만큼 옮김\n",
        "    printf(\"\\nHost -> Device: \");\n",
        "    printData <<<1, 10>>> (dDataPtr); // 1로 변화된 데이터\n",
        "\n",
        "    setData <<<1, 10>>> (dDataPtr); // 2로 세팅\n",
        "\n",
        "    cudaMemcpy(data, dDataPtr, sizeof(int)*10, cudaMemcpyDeviceToHost);\n",
        "    // gpu에서 cpu로 데이터를 옮김\n",
        "    printf(\"\\nDevice-> Host: \");\n",
        "    for (int i = 0; i < 10; i++) printf(\"%d\", data[i]);\n",
        "\n",
        "    cudaFree(dDataPtr);\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aXVyJcm3KPhH",
        "outputId": "befe76b1-89d9-4832-b7f3-c413304509a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting Memcpy.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -o Memcpy Memcpy.cu\n",
        "!./Memcpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9DDlrEaGKPjn",
        "outputId": "15fb1c76-acf6-482c-9a99-044ab7a58224"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data in device: 0000000000\n",
            "Host -> Device: 1111111111\n",
            "Device-> Host: 2222222222"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile Vector_Sum.cu\n",
        "#include \"cuda_runtime.h\"\n",
        "#include \"device_launch_parameters.h\"\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <string.h>\n",
        "#include <math.h>\n",
        "#include <iostream>\n",
        "#include <chrono>\n",
        "\n",
        "// #define NUM_DATA 1024\n",
        "#define NUM_DATA 134217728\n",
        "\n",
        "// Simple vector sum kernel\n",
        "__global__ void vecAdd(int* _a, int* _b, int* _c, int _size) {\n",
        "    int tID = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (tID < _size) _c[tID] = _a[tID] + _b[tID];\n",
        "    // 최대 블럭 스레드 번호는 tID 계산에서 고려되지 않음\n",
        "}\n",
        "\n",
        "int main() {\n",
        "\n",
        "    int *a, *b, *c, *hc; // Vectors on host\n",
        "    int *da, *db, *dc; // Vectors on device\n",
        "\n",
        "    int memSize = sizeof(int) * NUM_DATA;\n",
        "    printf(\"%d elements, memSize = %d bytes\\n\", NUM_DATA, memSize);\n",
        "\n",
        "    // memory-allocation on the host-side\n",
        "    a = new int[NUM_DATA]; memset(a, 0, memSize);\n",
        "    b = new int[NUM_DATA]; memset(b, 0, memSize);\n",
        "    c = new int[NUM_DATA]; memset(c, 0, memSize);\n",
        "    hc = new int[NUM_DATA]; memset(hc, 0, memSize);\n",
        "\n",
        "    // Data generation\n",
        "    for (int i = 0; i < NUM_DATA; i++) {\n",
        "        a[i] = rand() % 10;\n",
        "        b[i] = rand() % 10;\n",
        "    }\n",
        "\n",
        "    auto start_host = std::chrono::high_resolution_clock::now();\n",
        "\n",
        "    // vector sum on host (for performance comparision)\n",
        "    for (int i = 0; i < NUM_DATA; i++) {\n",
        "        hc[i] = a[i] + b[i];\n",
        "    }\n",
        "\n",
        "    auto end_host = std::chrono::high_resolution_clock::now();\n",
        "\n",
        "    std::chrono::duration<double> elapsed_host = end_host - start_host;\n",
        "    std::cout << \"Host Elapsed time: \" << elapsed_host.count() << \" seconds\" << std::endl;\n",
        "\n",
        "    // Memory allocation on device\n",
        "    cudaMalloc(&da, memSize); cudaMemset(da, 0, memSize);\n",
        "    cudaMalloc(&db, memSize); cudaMemset(db, 0, memSize);\n",
        "    cudaMalloc(&dc, memSize); cudaMemset(dc, 0, memSize);\n",
        "\n",
        "    // Data Copy : Host -> Device\n",
        "    cudaMemcpy(da, a, memSize, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(db, b, memSize, cudaMemcpyHostToDevice);\n",
        "\n",
        "    auto start_device = std::chrono::high_resolution_clock::now();\n",
        "\n",
        "    // kernel call\n",
        "    // vecAdd <<<1, NUM_DATA>>> (da, db, dc); // NUM_DATA가 1024이하일때만 thread block 생성 가능\n",
        "\n",
        "    dim3 dimGrid(ceil((float)NUM_DATA / 256), 1, 1);\n",
        "    dim3 dimBlock(256, 1, 1);\n",
        "    vecAdd <<< dimGrid, dimBlock >>> (da, db, dc, NUM_DATA);\n",
        "    cudaDeviceSynchronize(); // 디바이스가 수행중인 작업이 끝날 때까지 대기\n",
        "\n",
        "    auto end_device = std::chrono::high_resolution_clock::now();\n",
        "\n",
        "    std::chrono::duration<double> elapsed_device = end_device - start_device;\n",
        "    std::cout << \"Device Elapsed time: \" << elapsed_device.count() << \" seconds\" << std::endl;\n",
        "\n",
        "    //Copy results: Device -> Host\n",
        "    cudaMemcpy(c, dc, memSize, cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // Release device memory\n",
        "    cudaFree(da); cudaFree(db); cudaFree(dc);\n",
        "\n",
        "    // check results\n",
        "    bool result = true;\n",
        "    for (int i = 0; i < NUM_DATA; i++) {\n",
        "        if (hc[i] != c[i]) {\n",
        "            printf(\"[%d] The result is not matched! (%d, %d) \\n\"\n",
        "            , i, hc[i], c[i]);\n",
        "            result = false;\n",
        "        }\n",
        "    }\n",
        "\n",
        "    if(result) printf(\"GPU works well!\\n\");\n",
        "\n",
        "    // Release host memory\n",
        "    delete[] a; delete[] b; delete[] c;\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HYMGnIJvWXfG",
        "outputId": "4d95f187-5ca0-41bb-d5bf-3ddfde411029"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting Vector_Sum.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -o Vector_Sum Vector_Sum.cu\n",
        "!./Vector_Sum"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vta3iIirWXpm",
        "outputId": "9142f7e5-45a7-4288-e602-40e01ef6f839"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "134217728 elements, memSize = 536870912 bytes\n",
            "Host Elapsed time: 0.497378 seconds\n",
            "Device Elapsed time: 0.0064109 seconds\n",
            "GPU works well!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile thread_layout.cu\n",
        "#include \"cuda_runtime.h\"\n",
        "#include \"device_launch_parameters.h\"\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <string.h>\n",
        "\n",
        "__global__ void checkIndex(void) {\n",
        "    printf(\"threadIdx:(%d, %d, %d) blockIdx: (%d, %d, %d) blockDim:(%d, %d, %d), gridDim: (%d, %d, %d)\\n\",\n",
        "    threadIdx.x, threadIdx.y, threadIdx.z,\n",
        "    blockIdx.x, blockIdx.y, blockIdx.z,\n",
        "    blockDim.x, blockDim.y, blockDim.z,\n",
        "    gridDim.x, gridDim.y, gridDim.z);\n",
        "}\n",
        "\n",
        "int main() {\n",
        "\n",
        "    dim3 dimBlock(3, 1, 1); // or dimBlock(3)\n",
        "    dim3 dimGrid(2, 1, 1); // or dimGrid(2)\n",
        "\n",
        "    printf(\"dimGrid.x=%d dimGrid.y=%d dimGrid.z=%d\\n\", dimGrid.x, dimGrid.y, dimGrid.z);\n",
        "    printf(\"dimBlock.x=%d dimBlock.y=%d dimBlock.z=%d\\n\", dimBlock.x, dimBlock.y, dimBlock.z);\n",
        "\n",
        "    checkIndex<<<dimGrid, dimBlock>>> ();\n",
        "\n",
        "    // GPU가 커널 실행을 완료할 때까지 대기\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sH8FmzN7fu7x",
        "outputId": "98577eb9-12d6-4a90-e245-4ecd89684790"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing thread_layout.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -o thread_layout thread_layout.cu\n",
        "!./thread_layout"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vs6moXYPfvGC",
        "outputId": "5d88a9de-575f-469a-e0e2-fd55e633463c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dimGrid.x=2 dimGrid.y=1 dimGrid.z=1\n",
            "dimBlock.x=3 dimBlock.y=1 dimBlock.z=1\n",
            "threadIdx:(0, 0, 0) blockIdx: (0, 0, 0) blockDim:(3, 1, 1), gridDim: (2, 1, 1)\n",
            "threadIdx:(1, 0, 0) blockIdx: (0, 0, 0) blockDim:(3, 1, 1), gridDim: (2, 1, 1)\n",
            "threadIdx:(2, 0, 0) blockIdx: (0, 0, 0) blockDim:(3, 1, 1), gridDim: (2, 1, 1)\n",
            "threadIdx:(0, 0, 0) blockIdx: (1, 0, 0) blockDim:(3, 1, 1), gridDim: (2, 1, 1)\n",
            "threadIdx:(1, 0, 0) blockIdx: (1, 0, 0) blockDim:(3, 1, 1), gridDim: (2, 1, 1)\n",
            "threadIdx:(2, 0, 0) blockIdx: (1, 0, 0) blockDim:(3, 1, 1), gridDim: (2, 1, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile CUDA_definitions.cuh\n",
        "#ifndef CUDA_DEFINITIONS_CUH\n",
        "#define CUDA_DEFINITIONS_CUH\n",
        "\n",
        "// BLOCK ID\n",
        "#define BID_X blockIdx.x\n",
        "#define BID_Y blockIdx.y\n",
        "#define BID_Z blockIdx.z\n",
        "\n",
        "// Thread ID\n",
        "#define TID_X threadIdx.x\n",
        "#define TID_Y threadIdx.y\n",
        "#define TID_Z threadIdx.z\n",
        "\n",
        "// Dimension of a block\n",
        "#define Bdim_X blockDim.x\n",
        "#define Bdim_Y blockDim.y\n",
        "#define Bdim_Z blockDim.z\n",
        "\n",
        "// Dimension of a grid\n",
        "#define Gdim_X gridDim.x\n",
        "#define Gdim_Y gridDim.y\n",
        "#define Gdim_Z gridDim.z\n",
        "\n",
        "// global thread ID in blocks\n",
        "#define TID_IN_BLOCK (TID_Z*(Bdim_Y*Bdim_X) + TID_Y*Bdim_X + TID_X)\n",
        "\n",
        "// number of threads in a block\n",
        "#define NUM_THREAD_IN_BLOCK (Bdim_X*Bdim_Y*Bdim_Z)\n",
        "\n",
        "// global thread ID in grids\n",
        "#define GRID_1D_TID (BID_X * NUM_THREAD_IN_BLOCK) + TID_IN_BLOCK\n",
        "#define GRID_2D_TID (BID_Y * (Gdim_X * NUM_THREAD_IN_BLOCK) + GRID_1D_TID)\n",
        "#define GLOBAL_TID (BID_Z * (Gdim_Y * Gdim_X * NUM_THREAD_IN_BLOCK) + GRID_2D_TID)\n",
        "\n",
        "#endif\n",
        "\n",
        "// 나중에 필요시 #include CUDA_definitions.cuh 추가하면 됨(같은 디렉토리에서)"
      ],
      "metadata": {
        "id": "5cG5TtOOki_a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile MatAdd_G2D_B2D.cu\n",
        "// 2차원 그리드, 2차원 블록 스레드 레이아웃에서 크기가 1024이상인 대규모 행렬 합\n",
        "#include \"cuda_runtime.h\"\n",
        "#include \"device_launch_parameters.h\"\n",
        "//#include \"CUDA_definitions.cuh\"\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <math.h>\n",
        "\n",
        "__global__ void MatAdd_G2D_B2D\n",
        "(float* MatA, float* MatB, float* MatC, int ROW_SIZE, int COL_SIZE)\n",
        "{\n",
        "    unsigned int col = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "    unsigned int row = threadIdx.y + blockIdx.y * blockDim.y;\n",
        "    unsigned int index = row * COL_SIZE + col;\n",
        "\n",
        "    if (col < COL_SIZE && row < ROW_SIZE) MatC[index] = MatA[index] + MatB[index];\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    dim3 blockDim(32, 32); // 블럭당 최대 스레드 개수 1024\n",
        "    dim3 gridDim(ceil((float)COL_SIZE / blockDim.x), ceil((float)ROW_SIZE / blockDim.y));\n",
        "    MatAdd_G2D_B2D <<< gridDim, blockDim >>> (A, B, C, ROW_SIZE, COL_SIZE);\n",
        "    cudaDeviceSynchronize();\n",
        "    // A, B, C는 행렬 배열 포인터, ROW_SIZE, COL_SIZE는 전처리된 사이즈\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "id": "Rm9yPCqzkjJL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile MatAdd_G1D_B1D.cu\n",
        "// 1차원 그리드, 1차원 블록 스레드 레이아웃에서 크기가 1024이상인 대규모 행렬 합\n",
        "#include \"cuda_runtime.h\"\n",
        "#include \"device_launch_parameters.h\"\n",
        "//#include \"CUDA_definitions.cuh\"\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <math.h>\n",
        "\n",
        "__global__ void MatAdd_G1D_B1D\n",
        "(float* MatA, float* MatB, float* MatC, int ROW_SIZE, int COL_SIZE)\n",
        "{\n",
        "    unsigned int col = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "   if (col < COL_SIZE) {\n",
        "      for (int row = 0; row < ROW_SIZE; row++) {\n",
        "          int index = row * COL_SIZE + col;\n",
        "          MatC[index] = MatA[index] + MatB[index];\n",
        "      }\n",
        "   }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    dim3 blockDim(32); // 블럭당 최대 스레드 개수 1024\n",
        "    dim3 gridDim(ceil((float)COL_SIZE / blockDim.x));\n",
        "    MatAdd_G1D_B1D <<< gridDim, blockDim >>> (A, B, C, ROW_SIZE, COL_SIZE);\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "id": "TqUTrpq0Bffw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile MatAdd_G2D_B1D.cu\n",
        "// 2차원 그리드, 1차원 블록 스레드 레이아웃에서 크기가 1024이상인 대규모 행렬 합\n",
        "#include \"cuda_runtime.h\"\n",
        "#include \"device_launch_parameters.h\"\n",
        "//#include \"CUDA_definitions.cuh\"\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <math.h>\n",
        "\n",
        "__global__ void MatAdd_G2D_B1D\n",
        "(float* MatA, float* MatB, float* MatC, int ROW_SIZE, int COL_SIZE)\n",
        "{\n",
        "    unsigned int col = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "    unsigned int row = blockIdx.y;\n",
        "    unsigned int index = row * COL_SIZE + col;\n",
        "\n",
        "    if (col < COL_SIZE && row < ROW_SIZE) MatC[index] = MatA[index] + MatB[index];\n",
        "\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    dim3 blockDim(32); // 블럭당 최대 스레드 개수 1024\n",
        "    dim3 gridDim(ceil((float)COL_SIZE / blockDim.x), ROW_SIZE);\n",
        "    MatAdd_G2D_B1D <<< gridDim, blockDim >>> (A, B, C, ROW_SIZE, COL_SIZE);\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "id": "9640HliEcqsV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile Device_Query.cu\n",
        "\n",
        "#include \"cuda_runtime.h\"\n",
        "#include \"device_launch_parameters.h\"\n",
        "//#include \"helper_cuda.h\"\n",
        "\n",
        "#include <stdio.h>\n",
        "\n",
        "#define _1MB (1024*1024)\n",
        "\n",
        "\n",
        "int main() {\n",
        "    int ngpus; // number of gpus\n",
        "    cudaGetDeviceCount(&ngpus);\n",
        "    for (int i = 0; i < ngpus; i++) {\n",
        "        cudaDeviceProp devProp;\n",
        "\n",
        "        cudaGetDeviceProperties(&devProp, i);\n",
        "\n",
        "        printf(\"Device %d: %s\\n\", i, devProp.name);\n",
        "        printf(\"\\tCompute capability: %d.%d\\n\", devProp.major, devProp.minor);\n",
        "        printf(\"\\tThe number of streaming multiprocessors: %d\\n\", devProp.multiProcessorCount);\n",
        "        //printf(\"\\tThe number of CUDA cores: %d\\n\", _ConvertSMVer2Cores(devProp.major, devProp.minor) * devProp.multiProcessorCount);\n",
        "        printf(\"\\tGlobal memory size: %.2f MB\", (float)devProp.totalGlobalMem / _1MB);\n",
        "    }\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EabBWegXs1tF",
        "outputId": "4e499cfc-ce17-47d7-cebb-800c2498b9a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting Device_Query.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -o Device_Query Device_Query.cu\n",
        "!./Device_Query"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hIeV3cTfs19U",
        "outputId": "25ea7a90-ae17-4801-b1d0-32f2e94e8973"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device 0: Tesla T4\n",
            "\tCompute capability: 7.5\n",
            "\tThe number of streaming multiprocessors: 40\n",
            "\tGlobal memory size: 15102.06 MB"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile Matrix_Multiplcation.cu\n",
        "\n",
        "#include \"cuda_runtime.h\"\n",
        "#include \"device_launch_parameters.h\"\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <string.h>\n",
        "#include <math.h>\n",
        "\n",
        "#define BLOCK_SIZE 16\n",
        "\n",
        "__global__ void matMul_kernel_lowerThan1024ver\n",
        "(int* A, int* B, int* C, int m, int n, int k)\n",
        "// A, B, C는 행렬 포인터, A의 차원: m * k, B의 차원: k * n, C의 차원: m * n\n",
        "{\n",
        "    int row = threadIdx.x;\n",
        "    int col = threadIdx.y;\n",
        "    int index = row * n + col;\n",
        "\n",
        "    if (row >= m || col >= n) return;\n",
        "\n",
        "    C[index] = 0;\n",
        "    for (int offest = 0; offset < k; offset++) {\n",
        "        C[index] += A[row * k + offset] * B[col + offset * n];\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void matMul_kernel_higherThan1024ver\n",
        "(int* A, int* B, int* C, int m, int n, int k)\n",
        "// A, B, C는 행렬 포인터, A의 차원: m * k, B의 차원: k * n, C의 차원: m * n\n",
        "{\n",
        "    int row = (blockDim.x * blockIdx.x) + threadIdx.x;\n",
        "    int col = (blockDim.y * blockIdx.y) + threadIdx.y;\n",
        "    int index = row * n + col;\n",
        "\n",
        "    if (row >= m || col >= n) return;\n",
        "\n",
        "    C[index] = 0;\n",
        "    for (int offest = 0; offset < k; offset++) {\n",
        "        C[index] += A[row * k + offset] * B[col + offset * n];\n",
        "    }\n",
        "}\n",
        "\n",
        "int main(int argc, char* argv[]) {\n",
        "\n",
        "    int m,n,k;\n",
        "    m = atoi(argv[1]); n = atoi(argv[2]); k = atoi(argv[3]);\n",
        "\n",
        "    int sizeA = m * k;\n",
        "    int sizeB = k * n;\n",
        "    int sizeC = m * n;\n",
        "\n",
        "    int* dA, *dB, *dC;\n",
        "\n",
        "    // 1. Allocate device memory for dA, dB dC\n",
        "    cudaMalloc(&dA, sizeA * sizeof(int)); cudaMemset(dA, 0, sizeA * sizeof(int));\n",
        "    cudaMalloc(&dB, sizeB * sizeof(int)); cudaMemset(dB, 0, sizeB * sizeof(int));\n",
        "    cudaMalloc(&dC, sizeC * sizeof(int)); cudaMemset(dC, 0, sizeC * sizeof(int));\n",
        "\n",
        "    // 2. Send(Copy) tje input matrices to GPU (A -> dA, B -> dB)\n",
        "    cudaMemcpy(dA, A, sizeA * sizeof(int), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(dB, B, sizeB * sizeof(int), cudaMemcpyHostToDevice);\n",
        "\n",
        "    // 3. Set the thread layout\n",
        "    dim3 gridDim(ceil((float)m / BLOCK_SIZE), ceil((float)n / BLOCK_SIZE));\n",
        "    dim3 blockDim(BLOCK_SIZE, BLOCK_SIZE);\n",
        "\n",
        "    // 4. kernel call\n",
        "    matMul_kernel_higherThan1024ver <<< gridDim, blockDim >>> (dA, dB, dC, m, n, k);\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    // 5. Get(Copy) the result from GPU to host memory (dC  -> Cgpu)\n",
        "    cudaMemcpy(Cgpu, dC, sizeC * sizeof(int), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // 6. Release device memory space\n",
        "    cudaFree(dA);\n",
        "    cudaFree(dB);\n",
        "    cudaFree(dC);\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "id": "fGM0zMRT7PwV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile Matrix_Multiplcation_Shared.cu\n",
        "// 1024개의 스레드로 한 개 블록 이내의 경우 공유 메모리를 활용한 행렬 곱셈 예시\n",
        "#include \"cuda_runtime.h\"\n",
        "#include \"device_launch_parameters.h\"\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <math.h>\n",
        "\n",
        "#define BLOCK_SIZE 16\n",
        "#define ROW_SIZE 32\n",
        "#define COL_SIZE 32\n",
        "#define K_SIZE 128\n",
        "\n",
        "__global__ void matMul_kernel_shared\n",
        "(float* _A, float* _B, float* _C)\n",
        "// _A, _B, _C는 행렬 포인터, A 차원은 32 * 128, B 차원은 128 * 32, C 차원은 32 * 32\n",
        "{\n",
        "    int row = threadIdx.x;\n",
        "    int col = threadIdx.y;\n",
        "    int index = row * blockDim.y + col;\n",
        "\n",
        "    __shared__ float sA[ROW_SIZE][K_SIZE]; // 23*256*4 bytes = 16KB\n",
        "    __shared__ float sB[K_SIZE][COL_SIZE]; // 16KB\n",
        "    // 합계 32kb 는 48KB~96KB (GPU 공유 메모리)보다 작으므로 가능(공유 메모리 정적 할당)\n",
        "\n",
        "    if (row == 0) { // 첫 row 스레드가 read matrix B의 column을 다 공유 메모리에 넣음\n",
        "        for (int k = 0; k < K_SIZE; k++) sB[k][col] = _B[col + k * COL_SIZE];\n",
        "    }\n",
        "\n",
        "    if (col == 0) { // 첫 col 스레드가 read matrix A의 row을 다 공유 메모리에 넣음\n",
        "        for (int k = 0; k < K_SIZE; k++) sA[row][k] = _B[row * K_SIZE + k];\n",
        "    }\n",
        "\n",
        "    __syncthreads(); // wait until all threads load the matrix\n",
        "\n",
        "    float result = 0;\n",
        "    for (int k = 0; k < K_SIZE; k++) result += sA[row][k] * sB[k][col];\n",
        "    _C[index] = result;\n",
        "}\n",
        "\n",
        "\n",
        "int main(int argc, char* argv[]) {\n",
        "\n",
        "    int sizeA = ROW_SIZE * K_SIZE;\n",
        "    int sizeB = K_SIZE * COL_SIZE;\n",
        "    int sizeC = ROW_SIZE * COL_SIZE;\n",
        "\n",
        "    int* dA, *dB, *dC;\n",
        "\n",
        "    // 1. Allocate device memory for dA, dB dC\n",
        "    cudaMalloc(&dA, sizeA * sizeof(int)); cudaMemset(dA, 0, sizeA * sizeof(int));\n",
        "    cudaMalloc(&dB, sizeB * sizeof(int)); cudaMemset(dB, 0, sizeB * sizeof(int));\n",
        "    cudaMalloc(&dC, sizeC * sizeof(int)); cudaMemset(dC, 0, sizeC * sizeof(int));\n",
        "\n",
        "    // 2. Send(Copy) tje input matrices to GPU (A -> dA, B -> dB)\n",
        "    cudaMemcpy(dA, A, sizeA * sizeof(int), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(dB, B, sizeB * sizeof(int), cudaMemcpyHostToDevice);\n",
        "\n",
        "    // 3. Set the thread layout\n",
        "    dim3 gridDim(ceil((float)ROW_SIZE / BLOCK_SIZE), ceil((float)COL_SIZE / BLOCK_SIZE));\n",
        "    dim3 blockDim(BLOCK_SIZE, BLOCK_SIZE);\n",
        "\n",
        "    // 4. kernel call\n",
        "    matMul_kernel_shared <<< gridDim, blockDim >>> (dA, dB, dC);\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    // 5. Get(Copy) the result from GPU to host memory (dC  -> C)\n",
        "    cudaMemcpy(C, dC, sizeC * sizeof(int), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // 6. Release device memory space\n",
        "    cudaFree(dA);\n",
        "    cudaFree(dB);\n",
        "    cudaFree(dC);\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "id": "0lT4NcOyD4yD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile Matrix_Multiplcation_Shared_Large.cu\n",
        "\n",
        "#include \"cuda_runtime.h\"\n",
        "#include \"device_launch_parameters.h\"\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <string.h>\n",
        "#include <math.h>\n",
        "\n",
        "#define BLOCK_SIZE 16\n",
        "\n",
        "__global__ void matMul_kernel_shared_large\n",
        "(int* matA, int* matB, int* matC, int m, int n, int k)\n",
        "// A, B, C는 행렬 포인터, A의 차원: m * k, B의 차원: k * n, C의 차원: m * n\n",
        "{\n",
        "    int row = (blockDim.x * blockIdx.x) + threadIdx.x;\n",
        "    int col = (blockDim.y * blockIdx.y) + threadIdx.y;\n",
        "\n",
        "    int val = 0;\n",
        "    __shared__ int subA[BLOCK_SIZE][BLOCK_SIZE];\n",
        "    __shared__ int subB[BLOCK_SIZE][BLOCK_SIZE];\n",
        "\n",
        "    int localRow = threadIdx.x;\n",
        "    int localCol = threadIdx.y;\n",
        "\n",
        "    for (int bID = 0; bID < ceil((float)k / BLOCK_SIZE); bID++) {\n",
        "        int stride = bId * BLOCK_SIZE;\n",
        "\n",
        "        if (row >= m || stride + localCol >= k) subA[localRow][localCol] = 0;\n",
        "        else subA[localRow][localCol] = matA[row * k + (stride + localCol)];\n",
        "\n",
        "        if (col >= n || stride + localRow >= k) subB[localRow][localCol] = 0;\n",
        "        else subB[localRow][localCol] = matB[(stride + localRow) * n + col];\n",
        "\n",
        "        __syncthreads(); // 모든 데이터의 복사가 완료될 때까지 대기\n",
        "\n",
        "        for (int i = 0; i < BLOCK_SIZE; i++) {\n",
        "            val += subA[localRow][i] * subB[i][localCol];\n",
        "        } // 서브 블록 행렬 계산 (C(localRow, localCol))\n",
        "        __syncthreads(); // 모든 스레드 계산 완료 대기\n",
        "    }\n",
        "\n",
        "    if (row >= m || col >= n) return;\n",
        "\n",
        "    matC[row * n + col] = val;\n",
        "}\n",
        "\n",
        "int main(int argc, char* argv[]) {\n",
        "\n",
        "    int m,n,k; // 1024로 시도하기\n",
        "    m = atoi(argv[1]); n = atoi(argv[2]); k = atoi(argv[3]);\n",
        "\n",
        "    int sizeA = m * k;\n",
        "    int sizeB = k * n;\n",
        "    int sizeC = m * n;\n",
        "\n",
        "    int* dA, *dB, *dC;\n",
        "\n",
        "    // 1. Allocate device memory for dA, dB dC\n",
        "    cudaMalloc(&dA, sizeA * sizeof(int)); cudaMemset(dA, 0, sizeA * sizeof(int));\n",
        "    cudaMalloc(&dB, sizeB * sizeof(int)); cudaMemset(dB, 0, sizeB * sizeof(int));\n",
        "    cudaMalloc(&dC, sizeC * sizeof(int)); cudaMemset(dC, 0, sizeC * sizeof(int));\n",
        "\n",
        "    // 2. Send(Copy) tje input matrices to GPU (A -> dA, B -> dB)\n",
        "    cudaMemcpy(dA, A, sizeA * sizeof(int), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(dB, B, sizeB * sizeof(int), cudaMemcpyHostToDevice);\n",
        "\n",
        "    // 3. Set the thread layout\n",
        "    dim3 gridDim(ceil((float)m / BLOCK_SIZE), ceil((float)n / BLOCK_SIZE));\n",
        "    dim3 blockDim(BLOCK_SIZE, BLOCK_SIZE);\n",
        "\n",
        "    // 4. kernel call\n",
        "    matMul_kernel_shared_large <<< gridDim, blockDim >>> (dA, dB, dC, m, n, k);\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    // 5. Get(Copy) the result from GPU to host memory (dC  -> C)\n",
        "    cudaMemcpy(C, dC, sizeC * sizeof(int), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // 6. Release device memory space\n",
        "    cudaFree(dA);\n",
        "    cudaFree(dB);\n",
        "    cudaFree(dC);\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "id": "Zp30koySbnrG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile WarpSynchronization.cu\n",
        "\n",
        "#include \"cuda_runtime.h\"\n",
        "#include \"device_launch_parameters.h\"\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <string.h>\n",
        "#include <math.h>\n",
        "\n",
        "#define BLOCK_SIZE 64\n",
        "\n",
        "__global__ void syncWarp_test()\n",
        "{\n",
        "    int tID = threadIdx.x;\n",
        "    int warpID = (int) (tID / 32);\n",
        "    __shared__ int masterID[BLOCK_SIZE/32];\n",
        "\n",
        "    if (threadIdx.x % 32 == 0) {\n",
        "        masterID[warpID] = tID;\n",
        "    }\n",
        "    __syncwarp(); // intra-warp synchronization (barrier)\n",
        "\n",
        "    printf(\"[T%d] The master of our warp is %d\\n\", tID, masterID [warpID]);\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    syncWarp_test <<< 1, BLOCK_SIZE >>> ();\n",
        "    cudaDeviceSynchronize();  // Ensure the kernel completes before the program exits\n",
        "\n",
        "    cudaError_t err = cudaGetLastError();  // Check for kernel launch errors\n",
        "    if (err != cudaSuccess) {\n",
        "        printf(\"CUDA Error: %s\\n\", cudaGetErrorString(err));\n",
        "    }\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "id": "uTLYAv-d0QIo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08a5537f-8b51-4ca0-9538-e18fc7196503"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing WarpSynchronization.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -o WarpSynchronization WarpSynchronization.cu\n",
        "!./WarpSynchronization"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RzUMAlKFSmiC",
        "outputId": "177503e9-c156-4b39-e721-4d34c4af4374"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[T0] The master of our warp is 0\n",
            "[T1] The master of our warp is 0\n",
            "[T2] The master of our warp is 0\n",
            "[T3] The master of our warp is 0\n",
            "[T4] The master of our warp is 0\n",
            "[T5] The master of our warp is 0\n",
            "[T6] The master of our warp is 0\n",
            "[T7] The master of our warp is 0\n",
            "[T8] The master of our warp is 0\n",
            "[T9] The master of our warp is 0\n",
            "[T10] The master of our warp is 0\n",
            "[T11] The master of our warp is 0\n",
            "[T12] The master of our warp is 0\n",
            "[T13] The master of our warp is 0\n",
            "[T14] The master of our warp is 0\n",
            "[T15] The master of our warp is 0\n",
            "[T16] The master of our warp is 0\n",
            "[T17] The master of our warp is 0\n",
            "[T18] The master of our warp is 0\n",
            "[T19] The master of our warp is 0\n",
            "[T20] The master of our warp is 0\n",
            "[T21] The master of our warp is 0\n",
            "[T22] The master of our warp is 0\n",
            "[T23] The master of our warp is 0\n",
            "[T24] The master of our warp is 0\n",
            "[T25] The master of our warp is 0\n",
            "[T26] The master of our warp is 0\n",
            "[T27] The master of our warp is 0\n",
            "[T28] The master of our warp is 0\n",
            "[T29] The master of our warp is 0\n",
            "[T30] The master of our warp is 0\n",
            "[T31] The master of our warp is 0\n",
            "[T32] The master of our warp is 32\n",
            "[T33] The master of our warp is 32\n",
            "[T34] The master of our warp is 32\n",
            "[T35] The master of our warp is 32\n",
            "[T36] The master of our warp is 32\n",
            "[T37] The master of our warp is 32\n",
            "[T38] The master of our warp is 32\n",
            "[T39] The master of our warp is 32\n",
            "[T40] The master of our warp is 32\n",
            "[T41] The master of our warp is 32\n",
            "[T42] The master of our warp is 32\n",
            "[T43] The master of our warp is 32\n",
            "[T44] The master of our warp is 32\n",
            "[T45] The master of our warp is 32\n",
            "[T46] The master of our warp is 32\n",
            "[T47] The master of our warp is 32\n",
            "[T48] The master of our warp is 32\n",
            "[T49] The master of our warp is 32\n",
            "[T50] The master of our warp is 32\n",
            "[T51] The master of our warp is 32\n",
            "[T52] The master of our warp is 32\n",
            "[T53] The master of our warp is 32\n",
            "[T54] The master of our warp is 32\n",
            "[T55] The master of our warp is 32\n",
            "[T56] The master of our warp is 32\n",
            "[T57] The master of our warp is 32\n",
            "[T58] The master of our warp is 32\n",
            "[T59] The master of our warp is 32\n",
            "[T60] The master of our warp is 32\n",
            "[T61] The master of our warp is 32\n",
            "[T62] The master of our warp is 32\n",
            "[T63] The master of our warp is 32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile atomicAdd.cu\n",
        "\n",
        "#include \"cuda_runtime.h\"\n",
        "#include \"device_launch_parameters.h\"\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <string.h>\n",
        "#include <math.h>\n",
        "#include <iostream>\n",
        "#include <chrono>\n",
        "\n",
        "#define GRID_SIZE 128 * 4\n",
        "#define BLOCK_SIZE 1024\n",
        "\n",
        "__global__ void threadCounting_noSync(int *a)\n",
        "{\n",
        "    (*a)++;\n",
        "}\n",
        "\n",
        "__global__ void threadCounting_atomicGlobal(int *a)\n",
        "{\n",
        "    atomicAdd(a, 1);\n",
        "}\n",
        "\n",
        "__global__ void threadCounting_atomicShared(int *a)\n",
        "{\n",
        "    __shared__ int sa; // 블록 당 할당된 공유 메모리\n",
        "    if (threadIdx.x == 0) sa = 0;\n",
        "    // 대표 스레드에서 공유 메모리 초기화\n",
        "    __syncthreads(); // barrier for all initialization\n",
        "\n",
        "    atomicAdd(&sa, 1); //block-level counting\n",
        "\n",
        "    __syncthreads(); // barrier for all operations\n",
        "\n",
        "    if (threadIdx.x == 0) atomicAdd(a, sa);\n",
        "    // grid-level counting\n",
        "    // 각 블록에서 하나의 스레드만 원자 함수를 호출하므로\n",
        "    // 동기화 참여 스레드 수는 블록의 수와 같음\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int *noSyncKernels;\n",
        "    int host_noSync = 0;\n",
        "\n",
        "    cudaMalloc(&noSyncKernels, sizeof(int));\n",
        "    cudaMemset(noSyncKernels, 0, sizeof(int));\n",
        "\n",
        "    auto start_nosync = std::chrono::high_resolution_clock::now();\n",
        "\n",
        "    threadCounting_noSync <<< GRID_SIZE, BLOCK_SIZE >>> (noSyncKernels);\n",
        "    cudaDeviceSynchronize();  // Ensure the kernel completes before the program exits\n",
        "\n",
        "    auto end_nosync = std::chrono::high_resolution_clock::now();\n",
        "\n",
        "    cudaError_t err = cudaGetLastError();  // Check for kernel launch errors\n",
        "    if (err != cudaSuccess) {\n",
        "        printf(\"CUDA Error: %s\\n\", cudaGetErrorString(err));\n",
        "        return 1;\n",
        "    }\n",
        "\n",
        "    cudaMemcpy(&host_noSync, noSyncKernels, sizeof(int), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    std::chrono::duration<double> elapsed_nosync = end_nosync - start_nosync;\n",
        "\n",
        "    printf(\"[No sync] # of threads = %d\\n\", host_noSync);\n",
        "    std::cout << \"No Sync Elapsed time: \" << elapsed_nosync.count() * 1000 << \" ms\" << std::endl;\n",
        "\n",
        "    cudaFree(noSyncKernels);\n",
        "\n",
        "    ///////////////////////////////////////////////////////////////////\n",
        "\n",
        "    int *SyncKernels;\n",
        "    int host_Sync = 0;\n",
        "\n",
        "    cudaMalloc(&SyncKernels, sizeof(int));\n",
        "    cudaMemset(SyncKernels, 0, sizeof(int));\n",
        "\n",
        "    auto start_sync = std::chrono::high_resolution_clock::now();\n",
        "\n",
        "    threadCounting_atomicGlobal <<< GRID_SIZE, BLOCK_SIZE >>> (SyncKernels);\n",
        "    cudaDeviceSynchronize();  // Ensure the kernel completes before the program exits\n",
        "\n",
        "    auto end_sync = std::chrono::high_resolution_clock::now();\n",
        "\n",
        "    cudaError_t err2 = cudaGetLastError();  // Check for kernel launch errors\n",
        "    if (err2 != cudaSuccess) {\n",
        "        printf(\"CUDA Error: %s\\n\", cudaGetErrorString(err2));\n",
        "        return 1;\n",
        "    }\n",
        "\n",
        "    cudaMemcpy(&host_Sync, SyncKernels, sizeof(int), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    std::chrono::duration<double> elapsed_sync = end_sync - start_sync;\n",
        "\n",
        "    printf(\"[Atomic Global] # of threads = %d\\n\", host_Sync);\n",
        "    std::cout << \"Atomic Elapsed time: \" << elapsed_sync.count() * 1000 << \" ms\" << std::endl;\n",
        "\n",
        "    cudaFree(SyncKernels);\n",
        "\n",
        "    //////////////////////////////////////////////////////////////////\n",
        "\n",
        "    int *SyncSharedKernels;\n",
        "    int host_Sync_Shared = 0;\n",
        "\n",
        "    cudaMalloc(&SyncSharedKernels, sizeof(int));\n",
        "    cudaMemset(SyncSharedKernels, 0, sizeof(int));\n",
        "\n",
        "    auto start_sync_shared = std::chrono::high_resolution_clock::now();\n",
        "\n",
        "    threadCounting_atomicGlobal <<< GRID_SIZE, BLOCK_SIZE >>> (SyncSharedKernels);\n",
        "    cudaDeviceSynchronize();  // Ensure the kernel completes before the program exits\n",
        "\n",
        "    auto end_sync_shared = std::chrono::high_resolution_clock::now();\n",
        "\n",
        "    cudaError_t err3 = cudaGetLastError();  // Check for kernel launch errors\n",
        "    if (err3 != cudaSuccess) {\n",
        "        printf(\"CUDA Error: %s\\n\", cudaGetErrorString(err3));\n",
        "        return 1;\n",
        "    }\n",
        "\n",
        "    cudaMemcpy(&host_Sync_Shared, SyncSharedKernels, sizeof(int), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    std::chrono::duration<double> elapsed_sync_shared = end_sync_shared - start_sync_shared;\n",
        "\n",
        "    printf(\"[Atomic Shared] # of threads = %d\\n\", host_Sync_Shared);\n",
        "    std::cout << \"Shared Atomic Elapsed time: \" << elapsed_sync_shared.count() * 1000 << \" ms\" << std::endl;\n",
        "\n",
        "    cudaFree(SyncSharedKernels);\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OWWJzvS_czt1",
        "outputId": "6b9318eb-c5e3-4ccd-d4be-bd0c1b11f0fe"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting atomicAdd.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -o atomicAdd atomicAdd.cu\n",
        "!./atomicAdd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l1zH6VFGc3pV",
        "outputId": "4fca7227-384d-4d41-aa5d-197c8cedfbb6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[No sync] # of threads = 25\n",
            "No Sync Elapsed time: 0.149171 seconds\n",
            "[Atomic Global] # of threads = 524288\n",
            "Atomic Elapsed time: 6.9282e-05 seconds\n",
            "[Atomic Shared] # of threads = 524288\n",
            "Shared Atomic Elapsed time: 4.0275e-05 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile MultiStreamAsync.cu\n",
        "\n",
        "#include \"cuda_runtime.h\"\n",
        "#include \"device_launch_parameters.h\"\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <string.h>\n",
        "#include <math.h>\n",
        "#include <iostream>\n",
        "#include <chrono>\n",
        "\n",
        "#define NUM_BLOCK (128 * 1024)\n",
        "#define ARRAY_SIZE (1024 * NUM_BLOCK)\n",
        "#define NUM_STREAMS 4\n",
        "#define WORK_LOAD 256\n",
        "\n",
        "__global__ void myKernel(int *_in, int* _out)\n",
        "{\n",
        "    int tID = blockDim.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    int temp = 0;\n",
        "    int in = _in[tID];\n",
        "    for (int i = 0; i < WORK_LOAD; i++) {\n",
        "        temp = (temp + in % 5) % 10;\n",
        "    }\n",
        "    _out[tID] = temp;\n",
        "} // 단순 내부 연산 시키기\n",
        "\n",
        "int main() {\n",
        "\n",
        "    int *in = NULL, *out = NULL, *dIn = NULL, *dOut = NULL;\n",
        "\n",
        "    cudaMallocHost(&in, sizeof(int) * ARRAY_SIZE); // pinned memory\n",
        "    memset(in, 0, sizeof(int) * ARRAY_SIZE); // host 메모리 초기화\n",
        "\n",
        "    cudaMallocHost(&out, sizeof(int) * ARRAY_SIZE); // pinned memory\n",
        "    memset(out, 0, sizeof(int) * ARRAY_SIZE); // host 메모리 초기화\n",
        "\n",
        "    cudaMalloc(&dIn, sizeof(int) * ARRAY_SIZE);\n",
        "    cudaMalloc(&dOut, sizeof(int) * ARRAY_SIZE); // gpu global memoey 할당\n",
        "\n",
        "    for (int i = 0; i < ARRAY_SIZE; i++) in[i] = rand() % 10; // 배열 초기화\n",
        "\n",
        "    // single stream version (동기)\n",
        "    cudaMemcpy(dIn, in, sizeof(int) * ARRAY_SIZE, cudaMemcpyHostToDevice);\n",
        "    myKernel <<< NUM_BLOCK, 1024 >>> (dIn, dOut);\n",
        "    cudaMemcpy(out, dOut, sizeof(int) * ARRAY_SIZE, cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // multi-stream version\n",
        "    cudaStream_t stream[NUM_STREAMS]; // Non-Null 스트림 변수 선언\n",
        "    for (int i = 0; i < NUM_STREAMS; i++) cudaStreamCreate(&stream[i]); // 스트림 생성\n",
        "\n",
        "    int chunkSize = ARRAY_SIZE / NUM_STREAMS; // 스트림 당 데이터\n",
        "\n",
        "    for (int i = 0; i < NUM_STREAMS; i++) {\n",
        "        int offset = chunkSize * i;\n",
        "        cudaMemcpyAsync(dIn + offset, in + offset, sizeof(int) * chunkSize,\n",
        "            cudaMemcpyHostToDevice, stream[i]);\n",
        "    } // 각 스트림 당 호스트에서 디바이스로 메모리 옮김\n",
        "\n",
        "    for (int i = 0; i < NUM_STREAMS; i++) {\n",
        "        int offset = chunkSize * i;\n",
        "        myKernel <<< NUM_BLOCK / NUM_STREAMS, 1024, 0, stream[i] >>>\n",
        "            (dIn + offset, dOut + offset);\n",
        "    } // 각 스트림 당 해당 메모리 영역에 대해 커널 수행\n",
        "\n",
        "    for (int i = 0; i < NUM_STREAMS; i++) {\n",
        "        int offset = chunkSize * i;\n",
        "        cudaMemcpyAsync(out + offset, dOut + offset, sizeof(int) * chunkSize,\n",
        "            cudaMemcpyDeviceToHost, stream[i]);\n",
        "    } // 각 스트림 당 디바이스에서 호스트로 결과 메모리 옮김\n",
        "\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    for (int i = 0; i < NUM_STREAMS; i++) cudaStreamDestroy(stream[i]); // 스트림 제거\n",
        "\n",
        "    cudaFree(dIn);\n",
        "    cudaFree(dOut); // gpu 메모리 할당 해제\n",
        "    cudaFreeHost(in);\n",
        "    cudaFreeHost(out); // pinned memory 해제\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eDq69LHaKmbH",
        "outputId": "7ea0a1d2-f864-4ad9-fded-4f5ac8e83acc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting MultiStreamAsync.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -o MultiStreamAsync MultiStreamAsync.cu\n",
        "!./MultiStreamAsync"
      ],
      "metadata": {
        "id": "cnRJ9MP1KsCF"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile StreamEvent.cu\n",
        "// 비동기로 동작 가능한 멀티 스트림을 cuda event로 수행 시간을 측정한다.\n",
        "#include \"cuda_runtime.h\"\n",
        "#include \"device_launch_parameters.h\"\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <string.h>\n",
        "#include <math.h>\n",
        "#include <iostream>\n",
        "#include <chrono>\n",
        "\n",
        "#define NUM_BLOCK (128 * 1024)\n",
        "#define ARRAY_SIZE (1024 * NUM_BLOCK)\n",
        "#define NUM_STREAMS 4\n",
        "#define WORK_LOAD 256\n",
        "\n",
        "__global__ void myKernel(int *_in, int* _out)\n",
        "{\n",
        "    int tID = blockDim.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    int temp = 0;\n",
        "    int in = _in[tID];\n",
        "    for (int i = 0; i < WORK_LOAD; i++) {\n",
        "        temp = (temp + in % 5) % 10;\n",
        "    }\n",
        "    _out[tID] = temp;\n",
        "} // 단순 내부 연산 시키기\n",
        "\n",
        "int main() {\n",
        "\n",
        "    int *in = NULL, *out = NULL, *dIn = NULL, *dOut = NULL;\n",
        "\n",
        "    cudaMallocHost(&in, sizeof(int) * ARRAY_SIZE); // pinned memory\n",
        "    memset(in, 0, sizeof(int) * ARRAY_SIZE); // host 메모리 초기화\n",
        "\n",
        "    cudaMallocHost(&out, sizeof(int) * ARRAY_SIZE); // pinned memory\n",
        "    memset(out, 0, sizeof(int) * ARRAY_SIZE); // host 메모리 초기화\n",
        "\n",
        "    cudaMalloc(&dIn, sizeof(int) * ARRAY_SIZE);\n",
        "    cudaMalloc(&dOut, sizeof(int) * ARRAY_SIZE); // gpu global memoey 할당\n",
        "\n",
        "    for (int i = 0; i < ARRAY_SIZE; i++) in[i] = rand() % 10; // 배열 초기화\n",
        "\n",
        "    // multi-stream version\n",
        "    cudaStream_t stream[NUM_STREAMS]; // Non-Null 스트림 변수 선언 (배열)\n",
        "    cudaEvent_t start[NUM_STREAMS], end[NUM_STREAMS]; // cuda 이벤트 변수 선언 (이벤트 배열)\n",
        "\n",
        "    for (int i = 0; i < NUM_STREAMS; i++) {\n",
        "        cudaStreamCreate(&stream[i]); // 스트림 생성\n",
        "        cudaEventCreate(&start[i]); cudaEventCreate(&end[i]); // 이벤트 생성\n",
        "    }\n",
        "\n",
        "    int chunkSize = ARRAY_SIZE / NUM_STREAMS; // 스트림 당 데이터\n",
        "\n",
        "    int offset[NUM_STREAMS] = {0};\n",
        "    for (int i = 0; i < NUM_STREAMS; i++) offset[i] = chunkSize * i;\n",
        "\n",
        "    for (int i = 0; i < NUM_STREAMS; i++) {\n",
        "        cudaEventRecord(start[i], stream[i]); // cuda event 기록: cuda event를 stream에 넣음\n",
        "\n",
        "        cudaMemcpyAsync(dIn + offset[i], in + offset[i], sizeof(int) * chunkSize,\n",
        "            cudaMemcpyHostToDevice, stream[i]);\n",
        "    } // 각 스트림 당 호스트에서 디바이스로 메모리 옮김\n",
        "\n",
        "    for (int i = 0; i < NUM_STREAMS; i++) {\n",
        "        myKernel <<< NUM_BLOCK / NUM_STREAMS, 1024, 0, stream[i] >>>\n",
        "            (dIn + offset[i], dOut + offset[i]);\n",
        "    } // 각 스트림 당 해당 메모리 영역에 대해 커널 수행\n",
        "\n",
        "    for (int i = 0; i < NUM_STREAMS; i++) {\n",
        "        cudaMemcpyAsync(out + offset[i], dOut + offset[i], sizeof(int) * chunkSize,\n",
        "            cudaMemcpyDeviceToHost, stream[i]);\n",
        "\n",
        "        cudaEventRecord(end[i], stream[i]); // 각 스트림 당 end event를 넣음\n",
        "    } // 각 스트림 당 디바이스에서 호스트로 결과 메모리 옮김\n",
        "\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    for (int i = 0; i < NUM_STREAMS; i++) {\n",
        "        if (cudaEventQuery(start[i]) == cudaSuccess\n",
        "            && cudaEventQuery(end[i]) == cudaSuccess) {\n",
        "                float time = 0;\n",
        "                cudaEventElapsedTime(&time, start[i], end[i]);\n",
        "                printf(\"Stream[%d] : %f ms\\n\", i, time);\n",
        "            } // 이벤트가 성공적으로 일어났다면\n",
        "        else {\n",
        "            printf(\"Event has not occured!\");\n",
        "        }\n",
        "    }\n",
        "\n",
        "    for (int i = 0; i < NUM_STREAMS; i++) {\n",
        "        cudaStreamDestroy(stream[i]); // 스트림 제거\n",
        "        cudaEventDestroy(start[i]); cudaEventDestroy(end[i]); // 이벤트 제거\n",
        "    }\n",
        "\n",
        "    cudaFree(dIn);\n",
        "    cudaFree(dOut); // gpu 메모리 할당 해제\n",
        "    cudaFreeHost(in);\n",
        "    cudaFreeHost(out); // pinned memory 해제\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69DgZ-_2RT6o",
        "outputId": "89bc4b50-3918-46ae-d8ca-e74f89a67d75"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting StreamEvent.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -o StreamEvent StreamEvent.cu\n",
        "!./StreamEvent"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R7rpd_3URUFv",
        "outputId": "af3dfa3a-bcf0-47f1-e201-ab942f2bc036"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stream[0] : 147.220383 ms\n",
            "Stream[1] : 169.960220 ms\n",
            "Stream[2] : 192.696701 ms\n",
            "Stream[3] : 215.471710 ms\n"
          ]
        }
      ]
    }
  ]
}